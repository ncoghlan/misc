{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital Blasphemy Wallpaper Sync\n",
    "================================\n",
    "\n",
    "[Digital Blasphemy](http://digitalblasphemy.com) is a great site where the digital artist Ryan Bliss posts a wide variety of wallpapers for download.\n",
    "\n",
    "While a selection of the pieces are available for free, subscribing to the site provides access to all of the images at a variety of resolutions. Given how much I like Ryan's art, I signed up for both the [lifetime supporter](https://secure.digitalblasphemy.com/signup.shtml) subscription and for his [Patreon](https://www.patreon.com/dblasphemy?ty=h) to help ensure I'll have more artwork to download in the future :)\n",
    "\n",
    "I've long used a random selection of the Digital Blasphemy artwork as the desktop background on my personal laptop, but for a long time updating the available images was a matter of downloading the complete zip archives at the relevant resolutions, unzipping them to the appropriate location, and then going through them to delete the few that I know I don't like (or don't mind myself, but wouldn't be happy to have on-screen at a professional conference).\n",
    "\n",
    "Eventually, I decided to solve the problem in a more sensible way, by figuring out a way to automate the process of checking for images I didn't have (in the resolutions I care about) and downloading them to the right location.\n",
    "\n",
    "Packaging that up properly as a command line application would be a lot of work that wouldn't really help *me*, but by using an IPython notebook, I was able to convert my experimental code to see how I could retrieve the relevant data from the site directly into something that actually solved my original problem :)\n",
    "\n",
    "If the name Digital Blasphemy sounds vaguely familiar, it may be due to *this* image (or one of its earlier incarnations):\n",
    "\n",
    "![Flourescence (2009 version)](http://digitalblasphemy.com/graphics/fb/fluorescence2k93fb.jpg)\n",
    "\n",
    "Using the notebook\n",
    "--------------------\n",
    "\n",
    "UPDATE 2022: sadly, with the migration to the new web store on the DB site, this notebook doesn't work anymore. The migration makes the DB website itself quite a bit nicer, but the use of a JS frontend with a backend web API and AWS S3 image storage means that the notebook's naive HTML link scraping approach is no longer viable.\n",
    "\n",
    "UPDATE 2025: The image folder updater has been migrated from Jupyter to [Marimo](https://docs.marimo.io/) so images don't end up being inadvertently committed to source control. The web UI compatibility issue has been avoided by narrowing the scope to syncing image folder updates from a downloaded zip archive rather than querying the website directly.\n",
    "The updated script can be found in the [Github repo](https://github.com/ncoghlan/misc/tree/main/notebooks)\n",
    " \n",
    "NOTE: To grab an initial set of member-only images, I recommend using the zip archives Ryan publishes. This notebook is designed to handle collecting new images every few months, without adding back in any images you decided you didn't want, not for a bulk download of the entire gallery.\n",
    "\n",
    "1. Load the notebook using a Python 3 Jupyter kernel (other than the standard library, the only dependency is `requests`)\n",
    "2. Set the `LOCAL_MIRROR` global below to the destination directory\n",
    "3. Set the `RESOLUTIONS` global for the image resolutions you want to download\n",
    "4. Create an `access.cfg` file in your local mirror directory with your Digital Blasphemy login credentials (DB just uses HTTP Basic Auth to control access, and some authenticated pages are currently only available over HTTP, so assume any password you use here can be compromised in transit)\n",
    "5. Optionally, update the `BLOCKED` global to nominate particular images you don't want to download\n",
    "6. Run the whole notebook - the checked in version does a dry run by default\n",
    "7. If the dry run output looks sensible, change DRY_RUN to False and run the last two cells again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import configparser\n",
    "import requests\n",
    "import re\n",
    "\n",
    "DRY_RUN = True\n",
    "LOCAL_MIRROR = os.path.expanduser(\"~/Pictures/Digital Blasphemy/\")\n",
    "REMOTE_HOME_URL = 'http://digitalblasphemy.com/seeall.shtml'\n",
    "REMOTE_CONTENT_URL = 'http://digitalblasphemy.com/content/jpgs'\n",
    "RESOLUTIONS = [\"1440p\"]\n",
    "LOCAL_RES_DIRS = {res:os.path.join(LOCAL_MIRROR, res) for res in RESOLUTIONS}\n",
    "# Slightly hacky to use os.path.join on URLs, but it works well enough in this case\n",
    "REMOTE_RES_URLS = {res:os.path.join(REMOTE_CONTENT_URL, res) for res in RESOLUTIONS}\n",
    "\n",
    "# Basic config file for Digital Blasphemy login credentials\n",
    "CONFIG_FILE = os.path.join(LOCAL_MIRROR, \"access.cfg\")\n",
    "config = configparser.RawConfigParser()\n",
    "config.read(CONFIG_FILE)\n",
    "db_username = config.get(\"login\", \"username\")\n",
    "db_passwd = config.get(\"login\", \"password\")\n",
    "\n",
    "# Page retrieval helper\n",
    "\n",
    "def get_page(db_url):\n",
    "    \"\"\"Retrieve a Digital Blasphemy page using the configured credentials\"\"\"\n",
    "    return requests.get(db_url, auth=(db_username, db_passwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# To mangle a quote from a fine show:\n",
    "# \"They say never parse HTML with regular expressions,\n",
    "# but it is, on occasion, an expedient hack\" :)\n",
    "\n",
    "def iter_published_images():\n",
    "    content = get_page(REMOTE_HOME_URL).text\n",
    "    for m in re.finditer('href=\"/preview.shtml\\?i=(.*?)\"', content):\n",
    "        yield m.group(1)\n",
    "\n",
    "PUBLISHED = list(iter_published_images())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# I use my laptop for conference presentations\n",
    "# If I either don't really like a wallpaper or I'm\n",
    "# not happy displaying it at a professional\n",
    "# conference, I ensure I don't mirror it\n",
    "BLOCKED = (\"chamelea\", \"emblem\")\n",
    "\n",
    "# I also want to filter any images that are from the pickle jar\n",
    "# (experimental versions that aren't included in the main image index)\n",
    "ACCEPTABLE = set(name for name in PUBLISHED if not name.startswith(BLOCKED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def iter_remote_file_list(res):\n",
    "    content = get_page(REMOTE_RES_URLS[res]).text\n",
    "    # Complete hack to get the file list from the server index page\n",
    "    for m in re.finditer(r'<a href=\"(.*?)(%s\\.jpg)\">' % res, content):\n",
    "        candidate = m.group(1)\n",
    "        if candidate in ACCEPTABLE:\n",
    "            yield candidate + m.group(2)\n",
    "\n",
    "def get_remote_files(res):\n",
    "    return set(iter_remote_file_list(res))\n",
    "\n",
    "def get_local_files(res):\n",
    "    files = os.listdir(LOCAL_RES_DIRS[res])\n",
    "    return set(os.path.basename(f) for f in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_images_to_download(res):\n",
    "    remote = get_remote_files(res)\n",
    "    local = get_local_files(res)\n",
    "    return remote - local\n",
    "\n",
    "def download_image(source_url, dest_file, dryrun=True):\n",
    "    print(\"  Downloading {} -> {}\".format(source_url, dest_file))\n",
    "    if dryrun:\n",
    "        print(\"    Dry run only, skipping download\")\n",
    "        return\n",
    "    data = get_page(source_url).content\n",
    "    with open(dest_file, 'wb') as f:\n",
    "        f.write(data)\n",
    "    return len(data)\n",
    "\n",
    "# This assumes the local destination directory already exists\n",
    "def download_missing_images_for_res(res, dryrun=True):\n",
    "    source_url = REMOTE_RES_URLS[res]\n",
    "    dest_dir = LOCAL_RES_DIRS[res]\n",
    "    delay = 0.05 if dryrun else 0.5\n",
    "    images = get_images_to_download(res)\n",
    "    total = len(images)\n",
    "    if not total:\n",
    "        print(\"No {} images to download\".format(res))\n",
    "        return\n",
    "    print(\"{} {} images to be downloaded\".format(total, res))\n",
    "    downloaded_images = []\n",
    "    for i, image in enumerate(images, start=1):\n",
    "        print(\"Downloading {} image {}/{}\".format(res, i, total))\n",
    "        source = os.path.join(source_url, image)\n",
    "        dest = os.path.join(dest_dir, image)\n",
    "        download_image(source, dest, dryrun)\n",
    "        downloaded_images.append(dest)\n",
    "        time.sleep(delay) # Be nice to the server\n",
    "    return downloaded_images\n",
    "\n",
    "def download_missing_images(dryrun=True):\n",
    "    updated_resolutions = {}\n",
    "    for res in RESOLUTIONS:\n",
    "        images = download_missing_images_for_res(res, dryrun)\n",
    "        if images:\n",
    "            updated_resolutions[res] = images\n",
    "    return updated_resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "def show_images(filenames):\n",
    "    for filename in filenames:\n",
    "        display(Image(filename=filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 1440p images to download\n"
     ]
    }
   ],
   "source": [
    "downloaded = download_missing_images(dryrun=DRY_RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if downloaded and not DRY_RUN: show_images(downloaded[RESOLUTIONS[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
